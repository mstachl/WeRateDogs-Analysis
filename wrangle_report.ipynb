{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2166791",
   "metadata": {},
   "source": [
    "# Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a68d1",
   "metadata": {},
   "source": [
    "by Markus Stachl, 18.07.2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8ba64",
   "metadata": {},
   "source": [
    "## Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced5f6e",
   "metadata": {},
   "source": [
    "There were 3 datasets to be gathered from various sources and used for the analysis. Those were:\n",
    "- `tweets`: The dataset has been provided by Udacity as a csv-file. The dataset consists of tweets from the famous WeRateDog Twitter account containg tweet metadata like ID, tweeted text as well metadata like rating and dog name retreived from basic text processing.\n",
    "- `predictions`: The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) is hosted on Udacity's servers and was downloaded programmatically using the Requests library.\n",
    "- `tweets_extended`: A dataset containing detailled tweet metadata including number of favorites, number of retweets and extended user information. The dataset was retreived using the Twitter Tweepy API and read as JSON data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ccbcd",
   "metadata": {},
   "source": [
    "## Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a479ebb7",
   "metadata": {},
   "source": [
    "After reading each dataset into a Pandas dataframe, I started assessing them individually regarding quality and tidiness. This meant looking at the content of the tables and checking for validity, correctness and completeness. It also meant assessing table structures to ensure that every observation is modelled as a row, every feature is a column and every cell is a single value. Assessing the individual datasets was conducted via usage of pandas built-in functions like `.info()` and `.describe()`. After the assessment, I located the follwing quality and tidiness issues:\n",
    "\n",
    "### Tweets\n",
    "#### Quality\n",
    "- contains retweets\n",
    "- contains replies\n",
    "- contains non-dog related ratings, tweets without images (https://twitter.com/dog_rates/status/828361771580813312)\n",
    "- ratings (numerator and denominator) are not all base 10\n",
    "- tweets: wrong numerator/denominator for id=66628740622469529 and id=835246439529840640 and id=740373189193256964 and id=881633300179243008\n",
    "- tweets: outlier ratings for ids 680494726643068929, 778027034220126208, 786709082849828864  due to delimiter '.'\n",
    "- tweets: outlier ratings for ids 670842764863651840, 749981277374128128\t\n",
    "- wrong names \"a\",\"an\",\"light\",\"old\" et al.\n",
    "- contains invalid ids (see error_ids)\n",
    "- remove redundant/unnecesary columns\n",
    "#### Tidiness\n",
    "- wrong datatype for timestamp\n",
    "- index not set to tweet_id\n",
    "\n",
    "### Predictions\n",
    "#### Tidiness\n",
    "- predictions in separate table\n",
    "- multiple columns for dog stages\n",
    "\n",
    "### Tweets_extended\n",
    "#### Tidiness\n",
    "- two separate tables for tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8daaa76",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258a82d",
   "metadata": {},
   "source": [
    "Before performing any cleaning steps a copy of the original data was created in case a rollback is needed. Every issue was addressed individually by following a \"Define-Code-Test\"-methodology. Firstly, retweets and replies were removed from the dataset. Secondly, I assured correct structure by merging the three dataframes into one dataset by joining on the `tweet_id` and setting the correct datatype for the timestamp. Multiple representations of the dog stage were melted into a single column `stages`. WeRateDogs regularly uses a 1-10 scale, but adjusting the numerator and denominator for groups of dogs, like 121/110. The ratings were normalized to the 1-10 scale (11/10 in the given example). Furthermore, fake ratings were present in the dataset, like a dog with rating 1776/10 representing the US independence day, or Snoop Dogg with a rating of 420/10. These outliers were excluded from the dataset to ease future analysis. Lastly, invalid dog names were removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ecd587",
   "metadata": {},
   "source": [
    "Finally, the fully cleaned dataset was exported to a new file for future analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
